# Aegis Configuration
# Copy this to .env and fill in your values

# Proxy settings
AEGIS_HOST=127.0.0.1
AEGIS_PORT=8080

# Upstream LLM provider URL (e.g., OpenAI API)
AEGIS_UPSTREAM_URL=https://api.openai.com

# Encryption key for the PII vault (generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
AEGIS_VAULT_KEY=

# Logging level: DEBUG, INFO, WARN, ERROR
AEGIS_LOG_LEVEL=INFO

# Canary settings
AEGIS_CANARY_PREFIX=AEGIS-CANARY

# Guardrail settings
# Backend: auto | groq | onnx | huggingface (auto tries groq → onnx → hf)
AEGIS_GUARDRAIL_BACKEND=auto
AEGIS_GUARDRAIL_MODEL=meta-llama/Prompt-Guard-86M
AEGIS_INJECTION_THRESHOLD=0.90
AEGIS_JAILBREAK_THRESHOLD=0.85

# API Keys — used by guardrail backends and upstream providers
# Groq API key (used by guardrail groq backend for ultra-fast inference)
GROQ_API_KEY=""
# HuggingFace token (used by guardrail huggingface/onnx backends for model download)
HUGGINGFACEHUB_API_TOKEN=""
# OPENAI_API_KEY=
GOOGLE_API_KEY=''
API_KEY=""
TAVILY_API_KEY=""
PYDANTIC_AI_GATEWAY_API_KEY=""